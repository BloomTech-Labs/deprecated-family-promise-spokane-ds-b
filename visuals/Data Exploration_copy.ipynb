{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from root dir\n",
    "data = pd.read_csv('../All_data_with_exits.csv')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set view options\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original CSV Shape: \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recategorization\n",
    "\n",
    "\n",
    "Because the target is initially recorded in a very granular manner, the target labels will need to be recategorized to fit into the 5 Categories provided by stakeholder:\n",
    "\n",
    "- Permanent Exit\n",
    "- Temporary Exit\n",
    "- Emergency Shelter\n",
    "- Transitional Housing\n",
    "- Unknown/Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permanent Exit**\n",
    "\n",
    "- Staying or living with family, permanent tenure\n",
    "- Staying or living with friends, permanent tenure\n",
    "- Permanent housing (other than RRH) for formerly homeless persons\n",
    "- Rental by client with RRH or equivalent subsidy\n",
    "- Rental by client, no ongoing housing subsidy\n",
    "- Rental by client, other ongoing housing subsidy\n",
    "- Owned by client, no ongoing housing subsidy\n",
    "\n",
    "**Temporary Exit**  \n",
    "\n",
    "- Place not meant for habitation (e.g., a vehicle, an abandoned building, bus/train/subway, station/airport or anywhere outside)\n",
    "- Staying or living with family, temporary tenure (e.g., room, apartment or house)\n",
    "- Staying or living with friends, temporary tenure (e.g., room, apartment or house)\n",
    "- Hotel or Motel paid for without Emergency Shelter Voucher\n",
    "\n",
    "**Emergency Shelter**  \n",
    "\n",
    "- Emergency shelter, including hotel or motel paid for with emergency shelter voucher, or RHY-funded Host Home shelter \n",
    "\n",
    "**Transitional Housing**  \n",
    "\n",
    "- Transitional Housing for homeless persons (including homeless youth)\n",
    "- Safe Haven\n",
    "- Substance Abuse Treatment or Detox Center\n",
    "- Foster Care Home or Foster Care Group Home\n",
    "- Psychiatric Hospital or Other Psychiatric Facility\n",
    "\n",
    "**Unknown/Other**\n",
    "\n",
    "- No exit interview completed\n",
    "- Client refused\n",
    "- Other\n",
    "- Client doesn't know\n",
    "\n",
    "**RRH = Rapid Re-Housing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because pandas has a built in value mapping function that is more performant and consistent using a dictionary of this format, we are going with this dictionary structure rather than a more DRY dictionary with each entry as an element of a list with the category as the key.  \n",
    "e.g. `values_dict = {'Permanent Exit' : [some_value, some_value2]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use apply to assign values in dataframe to categories\n",
    "values_dict = {\n",
    "    \n",
    "    # Permanent Exits\n",
    "    'Staying or living with family, permanent tenure' : 'Permanent Exit',\n",
    "    'Staying or living with friends, permanent tenure' : 'Permanent Exit',\n",
    "    'Permanent housing (other than RRH) for formerly homeless persons' : 'Permanent Exit',\n",
    "    'Rental by client with RRH or equivalent subsidy' : 'Permanent Exit',\n",
    "    'Rental by client, no ongoing housing subsidy' : 'Permanent Exit',\n",
    "    'Rental by client, other ongoing housing subsidy' : 'Permanent Exit',\n",
    "    'Owned by client, no ongoing housing subsidy' : 'Permanent Exit',\n",
    "    \n",
    "    # Temporary Exits\n",
    "    'Staying or living with family, temporary tenure (e.g., room, apartment or house)' : 'Temporary Exit',\n",
    "    'Staying or living with friends, temporary tenure (e.g., room, apartment or house)' : 'Temporary Exit',\n",
    "    \n",
    "    # Emergency Shelter\n",
    "    'Emergency shelter, including hotel or motel paid for with emergency shelter voucher, or RHY-funded Host Home shelter' : 'Emergency Shelter',\n",
    "   \n",
    "    # Transitional Housing\n",
    "    'Transitional Housing for homeless persons (including homeless youth)' : 'Transitional Housing',\n",
    "    'Safe Haven' : 'Transitional Housing',\n",
    "    'Substance Abuse Treatment or Detox Center' : 'Transitional Housing',\n",
    "    'Foster Care Home or Foster Care Group Home' : 'Transitional Housing',\n",
    "    'Psychiatric Hospital or Other Psychiatric Facility' : 'Transitional Housing',\n",
    "   \n",
    "    # Unknown/Other\n",
    "    'Hotel or Motel paid for without Emergency Shelter Voucher' : 'Unknown/Other',\n",
    "    'Place not meant for habitation (e.g., a vehicle, an abandoned building, bus/train/subway station/airport or anywhere outside)' : 'Unknown/Other',\n",
    "    'No exit interview completed' : 'Unknown/Other',\n",
    "    'Client refused' : 'Unknown/Other',\n",
    "    'Other' : 'Unknown/Other',\n",
    "    'Client doesn\\'t know' : 'Unknown/Other',\n",
    "    np.NaN : 'Unknown/Other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that need to have dtype converted to datetime\n",
    "date_features = ['Enroll Date', 'Exit Date', 'CurrentDate', 'Date of First Contact (Beta)', \n",
    "                 'Date of First ES Stay (Beta)', 'Date of Last Contact (Beta)', \n",
    "                 'Date of Last ES Stay (Beta)', 'Engagement Date','Homeless Start Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features will artifacts remaining after filter application to text\n",
    "text_artifacts = ['RReferral Source',\n",
    "                  'RDate Status Determined',\n",
    "                  'REnroll Status',\n",
    "                  'RRunaway Youth',\n",
    "                  'RReason Why No Services Funded',\n",
    "                  'RSexual Orientation',\n",
    "                  'RLast Grade Completed',\n",
    "                  'RSchool Status',\n",
    "                  'REmployed Status',\n",
    "                  'RWhy Not Employed',\n",
    "                  'RType of Employment',\n",
    "                  'RLooking for Work',\n",
    "                  'RGeneral Health Status',\n",
    "                  'RDental Health Status',\n",
    "                  'RMental Health Status',\n",
    "                  'RPregnancy Status',\n",
    "                  'RPregnancy Due Date',\n",
    "                  'VLast Permanent Address',\n",
    "                  'VState',\n",
    "                  'VZip']\n",
    "\n",
    "# Dict comprehension to generate dict of fixed names\n",
    "rename_dict = {k: k[1:] for k in text_artifacts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Pipeline 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning has been split into multiple pipelines\n",
    "\n",
    "**Pipeline 1 Goals:**\n",
    "- Make column labels human readable and easily parsable\n",
    "- Enforce Data types\n",
    "- Create target exit destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(dataf):\n",
    "    '''Creates a copy of original dataframe to use in pipeline'''\n",
    "    return dataf.copy()\n",
    "\n",
    "def column_cleaner(dataf):\n",
    "    '''Takes in a dataframe and removes decimals from column names'''\n",
    "    dataf.columns = dataf.columns.str.replace(r'\\d+.', '')\n",
    "    return dataf\n",
    "\n",
    "def column_rename(dataf):\n",
    "    '''Fixes column name artifacts from string filter'''\n",
    "    dataf = dataf.rename(columns = rename_dict)\n",
    "    return dataf\n",
    "\n",
    "def column_strip(dataf):\n",
    "    '''Strips leading whitespace artifacting from RE'''\n",
    "    dataf.columns = dataf.columns.str.lstrip(' ')\n",
    "    return dataf\n",
    "\n",
    "def set_dtypes(dataf):\n",
    "    '''Converts date str to datetime objects in ordinal format'''\n",
    "    dataf[date_features] = dataf[date_features].apply(pd.to_datetime, infer_datetime_format=True)\n",
    "    for column in date_features:\n",
    "        dataf[column] = dataf[column].apply(dt.datetime.toordinal)\n",
    "    return dataf\n",
    "\n",
    "def add_target(dataf):\n",
    "    '''Adds each entry to one of the five target categories'''\n",
    "    dataf['Target Exit Destination'] = dataf['Exit Destination'].map(values_dict)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipeline1 = (df\n",
    "    .pipe(start_pipeline)\n",
    "    .pipe(column_cleaner)\n",
    "    .pipe(column_rename)\n",
    "    .pipe(column_strip)\n",
    "    .pipe(set_dtypes)\n",
    "    .pipe(add_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 Results Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for enforcement of datetime dtype\n",
    "for column in date_features:\n",
    "    print(df_pipeline1[column].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipeline1['Target Exit Destination'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pipeline1.shape)\n",
    "df_pipeline1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Cleaning & Pipeline\n",
    "Cleaning has been split into multiple pipelines\n",
    "\n",
    "**Pipeline 2 Goals:**\n",
    "- Remove columns in the dataframe with high incidence of null values\n",
    "- Remove columns that are contextually irrelevant to modeling\n",
    "- Data re-binning\n",
    "- Enforce column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be removed from feature selection due to not exisitng in the intake data\n",
    "not_in_intake = ['Utilization Tracking Method (Invalid)',\n",
    "                 'Federal Grant Programs',\n",
    "                 'Client Location',\n",
    "                 'Engagement Date',\n",
    "                 'Days Enrolled Until Engagement Date',\n",
    "                 'RRH | Most Recent Enrollment',\n",
    "                 'Coordinated Entry | Most Recent Enrollment',\n",
    "                 'Emergency Shelter | Most Recent Enrollment',\n",
    "                 'Bed Nights During Report Period',\n",
    "                 'Count of Bed Nights - Entire Episode',\n",
    "                 'Chronic Homeless Status_vHMISDatAssessment',\n",
    "                 'Chronic Homeless Status_EvaluatevHMIS&HMISDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be removed from feature selection for reasons described in column_removal_documentation.md\n",
    "columns_not_selected = ['Current Age',\n",
    "                        'Birthdate Quality',\n",
    "                        'Information Release Status',\n",
    "                        'InfoReleaseNo',\n",
    "                        'Client Record Restricted',\n",
    "                        'Contact Services',\n",
    "                        'Date of Last Contact (Beta)',\n",
    "                        'Date of First Contact (Beta)',\n",
    "                        'Chronic Homeless Status',\n",
    "                        'Exit Destination',\n",
    "                        'Personal ID',\n",
    "                        'Household ID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_need_testing = ['School Status', \n",
    "                        'Date of Last ES Stay (Beta)', \n",
    "                        'Date of First ES Stay (Beta)',\n",
    "                        'Non-Cash Benefit Count',\n",
    "                        'Non-Cash Benefit Count at Exit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 2\n",
    "def start_pipeline(dataf):\n",
    "    '''Creates a copy of original dataframe to use in pipeline'''\n",
    "    return dataf.copy()\n",
    "\n",
    "def replace_values(dataf):\n",
    "    '''Takes columns in column_impute_list and replaces missing and unknown \n",
    "    values with \"Unknown\"'''\n",
    "    # List of columns that needs values consolidated and replaced\n",
    "    column_replace_list = ['Race' , 'Ethnicity' , 'Length of Stay']\n",
    "    # List of values to replace with \"Unknown\"\n",
    "    value_replace_list = ['Client refused','Client doesn\\'t know', 'Data not collected', np.NaN]\n",
    "    for column in column_replace_list:\n",
    "        dataf[column].replace(value_replace_list, 'Unknown', inplace=True)\n",
    "    return dataf \n",
    "\n",
    "def remove_null_columns(dataf):\n",
    "    '''Removes columns with null incidence greater than threshold'''\n",
    "    # Set null threshold based on %\n",
    "    threshold = 0.90 * dataf.shape[0]\n",
    "    # Create a dictionary of the number of null values in each column\n",
    "    null_count_dict = dataf.isnull().sum().to_dict()\n",
    "    # Create a list of column labels that >= threshold\n",
    "    null_columns_list = [entry for entry in null_count_dict if null_count_dict[entry] >= threshold]\n",
    "    # Drop columns in null_columns_list\n",
    "    dataf.drop(columns = null_columns_list, inplace=True)\n",
    "    return dataf\n",
    "\n",
    "def remove_one_value_columns(dataf):\n",
    "    '''Removes columns with a cardinality of 1'''\n",
    "    # Create a dictionary of the number of null values in each column\n",
    "    nunique_count_dict = dataf.nunique(dropna=False).to_dict()\n",
    "    # Create a list of column labels that >= threshold\n",
    "    nunique_columns_list = [entry for entry in nunique_count_dict if nunique_count_dict[entry] == 1]\n",
    "    # Drop columns in null_columns_list\n",
    "    dataf.drop(columns = nunique_columns_list, inplace=True)\n",
    "    return dataf\n",
    "\n",
    "def remove_final_columns(dataf):\n",
    "    '''Removes columns that either do not appear in the intake or are selected for modeling'''\n",
    "    dataf = dataf.drop(columns=(not_in_intake + columns_not_selected))\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Pipeline 2\n",
    "df_pipeline2 = (df_pipeline1\n",
    "    .pipe(start_pipeline)\n",
    "    .pipe(replace_values)\n",
    "    .pipe(remove_null_columns)\n",
    "    .pipe(remove_one_value_columns)\n",
    "    .pipe(remove_final_columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 Results Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipeline2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipeline2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Visualizations  \n",
    "\n",
    "Final Visualizations will need to be formatted with proper object usage and syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Distribution\n",
    "df_pipeline2['Target Exit Destination'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic scatterplots\n",
    "sns.scatterplot(data=df_pipeline2, y='Target Exit Destination', x='Income Total at Entry').set_title('Exit Destination vs. Income Total at Entry');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of income columns\n",
    "income = ['Earned Income',\n",
    "          'Supplemental Security Income', \n",
    "          'Social Security Disability Income', \n",
    "          'VA Disability Compensation', \n",
    "          'Private Disability Income', \n",
    "          'Workers Compensation', \n",
    "          'TANF', \n",
    "          'General Assistance' ,\n",
    "          'Child Support', \n",
    "          'Other Income']\n",
    "\n",
    "\n",
    "# # Write a function that calculates the sum of these columns\n",
    "# dataf['Income Total'] = df.loc[income].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 3\n",
    "\n",
    "def start_pipeline(dataf):\n",
    "    '''Creates a copy of original dataframe to use in pipeline'''\n",
    "    return dataf.copy()\n",
    "\n",
    "def income_sum(dataf):\n",
    "    '''Creates a column that is the sum of each person\\'s income'''\n",
    "    dataf['Income Total'] = dataf.loc[income].sum(axis=1)\n",
    "    return dataf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Pipeline 3\n",
    "df_pipeline3 = (df_pipeline2\n",
    "    .pipe(start_pipeline)\n",
    "#     .pipe(income_sum)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pipeline3.shape)\n",
    "df_pipeline3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['CaseMembers','Race', 'Ethnicity', \n",
    "#             'Current Age', 'Gender', 'Length of Stay', \n",
    "#             'Days Enrolled in Project','Household Type', \n",
    "#             'Barrier Count at Entry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Target Exit Destination'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pipeline3.drop(columns=[target])\n",
    "y = df_pipeline3[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test, Validation Split\n",
    "\n",
    "# First split : Train, Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Second split : Train, Val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Modeling Strategy: \n",
    "- Implement SKL pipeline to add modularity to workflow\n",
    "- Begin with random forest implementation\n",
    "- Update model choices using combinations of cross-validation, loss metrics, hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for random forest model\n",
    "random_forest_model = Pipeline([('ord', OrdinalEncoder()),\n",
    "                                ('imputer', SimpleImputer()),\n",
    "                                ('classifier', RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42, verbose=1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation: \", random_forest_model.score(X_val, y_val))\n",
    "print(\"Test: \", random_forest_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for classification report metrics\n",
    "y_true = y_val\n",
    "y_pred = random_forest_model.predict(X_val)\n",
    "target_names = ['Permanent Exit', 'Temporary Exit', 'Transitional Housing', 'Emergency Shelter' , 'Unknown/Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Serialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the classifier step from the pipeline\n",
    "clf = random_forest_model['classifier']\n",
    "joblib_file = \"randomforest_modelv1.pkl\"\n",
    "joblib.dump(clf, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(random_forest_model, \"randomforest_model.joblib\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pipeline3.to_csv(\"../All_data_with_exits_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pkl = pd.read_pickle(r'randomforest_modelv1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_job = joblib.load('randomforest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labs30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup:\n",
    "import eli5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "# Plot:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning:\n",
    "from catboost import CatBoostClassifier\n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Interpretation:\n",
    "from eli5 import show_weights\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from pdpbox.pdp import pdp_isolate, pdp_interact, pdp_plot, pdp_interact_plot\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipeline3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"CaseMembers\"\n",
    "\n",
    "isolated = pdp_isolate(\n",
    "    model=model_job,\n",
    "    dataset=X_val,\n",
    "    model_features=X_val.columns,\n",
    "    feature = feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_plot(isolated[:1], feature_name=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(df_pipeline1['Household ID'].value_counts())\n",
    "sns.histplot(test_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pipeline3.shape)\n",
    "df_pipeline3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pipeline3[:500].to_csv(\"../visuals/All_data_with_exits_cleaned_500r.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
